{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aida-zw/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pickle as pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Predictor(object):\n",
    "    def __init__(self):\n",
    "        \n",
    "        #path_io = \"predictor/\"\n",
    "        path_io = \"/home/aida-zw/Desktop/ws/notebook/io/\"\n",
    "        \n",
    "        df_accu_dir = path_io + \"accu.csv\"\n",
    "        df_articles_dir = path_io + \"law.csv\"\n",
    "\n",
    "        self.df_accu = pd.read_csv(df_accu_dir)\n",
    "        self.df_articles = pd.read_csv(df_articles_dir)\n",
    "\n",
    "        self.n_classes_accu = 118\n",
    "        self.n_classes_articles = 94\n",
    "\n",
    "        self.model_dir_accu = path_io + 'model_accu/'\n",
    "        self.model_dir_articles = path_io + 'model_articles/'\n",
    "\n",
    "        label_encoder_dir_accu = path_io + 'label_encoder_accu.pickle'\n",
    "        label_encoder_dir_articles = path_io + 'label_encoder_articles.pickle'\n",
    "\n",
    "        hub_dir = path_io + 'google_nnlm-zh-dim128-with-normalization_1'\n",
    "\n",
    "\n",
    "        with open(label_encoder_dir_accu, 'rb') as handle:\n",
    "            self.label_encoder_accu = pickle.load(handle)\n",
    "\n",
    "\n",
    "        with open(label_encoder_dir_articles, 'rb') as handle:\n",
    "            self.label_encoder_articles = pickle.load(handle)\n",
    "        \n",
    "        self.feature_columns = [hub.text_embedding_column('fact',hub_dir , trainable=True)]\n",
    "\n",
    "        self.model_accu = tf.estimator.DNNClassifier([512, 512, 128],\n",
    "                                                     feature_columns=self.feature_columns, \n",
    "                                                     n_classes=self.n_classes_accu, \n",
    "                                                     model_dir=self.model_dir_accu)\n",
    "        \n",
    "        self.model_articles = tf.estimator.DNNClassifier([512, 512, 128],\n",
    "                                                     feature_columns=self.feature_columns, \n",
    "                                                     n_classes=self.n_classes_articles, \n",
    "                                                     model_dir=self.model_dir_articles)\n",
    "    \n",
    "    def predict(self, content):\n",
    "\n",
    "        test_data = pd.DataFrame({'fact': content})\n",
    "        ans_accusation, ans_accusation_arr = self.predict_accu(test_data)\n",
    "        ans_articles, ans_articles_arr = self.predict_articles(test_data)\n",
    "        ans_imprisonment = self.predict_time(test_data)\n",
    "#         ans = pd.DataFrame({'accusation':ans_accusation, 'articles': ans_articles, 'imprisonment':ans_imprisonment,\n",
    "#                            'accusation_arr':ans_accusation_arr, 'articles_arr': ans_articles_arr})\n",
    "        ans = pd.DataFrame({'accusation':ans_accusation, 'articles': ans_articles, 'imprisonment':ans_imprisonment})\n",
    "        dict_ans = ans.to_dict(orient='records')\n",
    "        return dict_ans\n",
    "        \n",
    "        \n",
    "    def predict_time(self, test_data):\n",
    "        time = []\n",
    "        for i in range(test_data.shape[0]):\n",
    "            time.append(7)\n",
    "        return time\n",
    "    \n",
    "    \n",
    "    def predict_articles(self, test_data):\n",
    "        test_input_fn = tf.estimator.inputs.pandas_input_fn(\n",
    "            test_data, \n",
    "            None, \n",
    "            shuffle=False, \n",
    "            batch_size=128)\n",
    "        result = list(self.model_articles.predict(input_fn=test_input_fn))\n",
    "        result_class = np.array([p['classes'][0] for p in result])\n",
    "        prediction_arr = self.label_encoder_articles.inverse_transform(result_class.astype(int))\n",
    "        articles_num = self.get_articles_num(self.df_articles, prediction_arr)\n",
    "        return articles_num, prediction_arr\n",
    "    \n",
    "    def predict_accu(self, test_data):\n",
    "        test_input_fn = tf.estimator.inputs.pandas_input_fn(\n",
    "            test_data, \n",
    "            None, \n",
    "            shuffle=False, \n",
    "            batch_size=128)\n",
    "        result = list(self.model_accu.predict(input_fn=test_input_fn))\n",
    "        result_class = np.array([p['classes'][0] for p in result])\n",
    "        prediction_arr = self.label_encoder_accu.inverse_transform(result_class.astype(int))\n",
    "        accusation_num = self.get_accusation_num(self.df_accu, prediction_arr)\n",
    "        \n",
    "        return accusation_num, prediction_arr\n",
    "    \n",
    "    def get_accusation_num(self, df_accu, arr):\n",
    "        keys = df_accu.accusation.tolist()\n",
    "        keys = [x.strip() for x in keys]\n",
    "        values = df_accu.accusation_num.tolist()\n",
    "        dict_accu = dict(zip(keys, values))\n",
    "\n",
    "        accusation_num = list()\n",
    "        for accu in arr:\n",
    "            accusation_num.append([dict_accu.get(x.replace(\"'\", \"\").strip()) for x in accu.split(',')])\n",
    "        return accusation_num\n",
    "    \n",
    "    def get_articles_num(self, df_articles, arr):\n",
    "        keys = df_articles.articles.tolist()\n",
    "        values = df_articles.articles_num.tolist()\n",
    "        dict_articles = dict(zip(keys, values))\n",
    "\n",
    "        articles_num = list()\n",
    "        for art in arr:\n",
    "            articles_num.append([dict_articles.get(int(x)) for x in art.split(',')])\n",
    "        return articles_num\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dry run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/home/aida-zw/Desktop/ws/notebook/io/model_accu/', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7faa9069e710>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/home/aida-zw/Desktop/ws/notebook/io/model_articles/', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7faa9069e908>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Initialize variable dnn/input_from_feature_columns/input_layer/fact_hub_module_embedding/module/embeddings/part_0:0 from checkpoint b'/home/aida-zw/Desktop/ws/notebook/io/google_nnlm-zh-dim128-with-normalization_1/variables/variables' with embeddings\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /home/aida-zw/Desktop/ws/notebook/io/model_accu/model.ckpt-3000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aida-zw/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Initialize variable dnn/input_from_feature_columns/input_layer/fact_hub_module_embedding/module/embeddings/part_0:0 from checkpoint b'/home/aida-zw/Desktop/ws/notebook/io/google_nnlm-zh-dim128-with-normalization_1/variables/variables' with embeddings\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /home/aida-zw/Desktop/ws/notebook/io/model_articles/model.ckpt-3000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aida-zw/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "path = \"/home/aida-zw/Desktop/ws/Data/sample/\"\n",
    "def read_data(csv_file, path):\n",
    "    df = pd.read_csv(path + csv_file)\n",
    "    df = df[['fact', 'accusation', 'relevant_articles', 'imprisonment']]\n",
    "    return df\n",
    "\n",
    "test_data = read_data('data_test.csv', path)\n",
    "content = test_data['fact'].tolist()    \n",
    "    \n",
    "model = Predictor()\n",
    "\n",
    "result = model.predict(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
